{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Evaluating Recommender Systems \n",
    "\n",
    "## Surprise \n",
    "\n",
    "[Surprise](http://surpriselib.com/) is a Python package that is desiged to aid the testing of Recommender Systems. We will use this, along with some extra packages developed from the Frank Kane Book to explore some popular datasets and use about the different evaluation metrics we saw in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install \n",
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code examples are developed from materials for \n",
    "#Building Recommender Systems Book by Frank Kane https://sundog-education.com/recsys/\n",
    "from MovieLens import MovieLens\n",
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from RecommenderMetrics import RecommenderMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens\n",
    "\n",
    "The [MovieLens dataset](https://grouplens.org/datasets/movielens/) is a collection of film ratings from the [MovieLens](https://movielens.org/) site. \n",
    "\n",
    "It contains information on ratings, genres and tags and comes in various sizes, up to __25 million__. \n",
    "\n",
    "\n",
    "\n",
    "### Explore the .csv files\n",
    "\n",
    "Use __Pandas__ to investigate the 4 datasets in the folder ``ml-latest-small`` and take a preview of the data in each __.csv__ file. \n",
    "\n",
    "Think about:\n",
    "\n",
    "\n",
    "1. What data is contained?\n",
    "\n",
    "2. How much variation is there? \n",
    "\n",
    "3. How would this data be useful for recommending new films to users?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the files \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper \n",
    "\n",
    "We have a smaller version thats easier and quicker for testing with. We'll use this ``MovieLens`` wrapper class to load the data and pass it to the Recommenders we build, and the for use with the evaluation metrics we get from ``Surprise``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = MovieLens()\n",
    "data = ml.loadMovieLensLatestSmall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running some real tests!\n",
    "\n",
    "For the questions below, run the code and the see if you can understand what is being calculated and what the results mean in terms of whether the Recommender System is working well. \n",
    "\n",
    "If you want to look at the code the the __Recommender Metrics__, you can look in the ``RecommenderMetrics.py`` file. You might not understand it all but its good practise to start exploring other peoples libraries!\n",
    "\n",
    "\n",
    "### Accuracy \n",
    "\n",
    "The code below uses an ``SVD`` model (more on that later) to build a Recommender system and uses ``Surprise`` to evaluate the accuracy of the trained model. \n",
    "\n",
    "Run the code and see if you can answer these questions\n",
    "\n",
    "\n",
    "1. What does the RMSE score mean?\n",
    "\n",
    "2. What does the MAE score mean?\n",
    "\n",
    "3. Why do you think they are different? What does this tell us?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing movie popularity ranks so we can measure novelty later...\")\n",
    "rankings = ml.getPopularityRanks()\n",
    "\n",
    "print(\"\\nComputing item similarities so we can measure diversity later...\")\n",
    "fullTrainSet = data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "simsAlgo = KNNBaseline(sim_options=sim_options)\n",
    "simsAlgo.fit(fullTrainSet)\n",
    "\n",
    "print(\"\\nBuilding recommendation model...\")\n",
    "trainSet, testSet = train_test_split(data, test_size=.25, random_state=1)\n",
    "\n",
    "algo = SVD(random_state=10)\n",
    "algo.fit(trainSet)\n",
    "\n",
    "print(\"\\nComputing recommendations...\")\n",
    "predictions = algo.test(testSet)\n",
    "\n",
    "print(\"\\nEvaluating accuracy of model...\")\n",
    "print(\"RMSE: \", RecommenderMetrics.RMSE(predictions))\n",
    "print(\"MAE: \", RecommenderMetrics.MAE(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit Rate\n",
    "\n",
    "Below, we use ``Surprise`` to calculate the __Hit Rate__ for our model. \n",
    "\n",
    "Can you describe what each step does and how to interpret the results at the end? You might want to consider\n",
    "\n",
    "1. What does the hit rate evaluate?\n",
    "\n",
    "2. What is in the trainSet and testSet?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set aside one rating per user for testing\n",
    "LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
    "\n",
    "for trainSet, testSet in LOOCV.split(data):\n",
    "    print(\"Computing recommendations with leave-one-out...\")\n",
    "    \n",
    "    # Train model without left-out ratings\n",
    "    algo.fit(trainSet)\n",
    "\n",
    "    # Predicts ratings for left-out ratings only\n",
    "    print(\"Predict ratings for left-out set...\")\n",
    "    leftOutPredictions = algo.test(testSet)\n",
    "\n",
    "    # Build predictions for all ratings not in the training set\n",
    "    print(\"Predict all missing ratings...\")\n",
    "    bigTestSet = trainSet.build_anti_testset()\n",
    "    allPredictions = algo.test(bigTestSet)\n",
    "\n",
    "    # Compute top 10 recs for each user\n",
    "    print(\"Compute top 10 recs per user...\")\n",
    "    topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n=10)\n",
    "\n",
    "    # See how often we recommended a movie the user actually rated\n",
    "    print(\"\\nHit Rate: \", RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Diversity and Novelty\n",
    "\n",
    "Finally, lets look at some metrics beyond performance. Can you answer these questions? \n",
    "\n",
    "1. What does the diversity score tell us? Is this particular value a good or a bad thing?\n",
    "\n",
    "2. What does the novelty score tell us? Again, is this a good or a bad thing for our recommender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing complete recommendations, no hold outs...\")\n",
    "algo.fit(fullTrainSet)\n",
    "bigTestSet = fullTrainSet.build_anti_testset()\n",
    "allPredictions = algo.test(bigTestSet)\n",
    "topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n=10)\n",
    "\n",
    "# Measure diversity of recommendations:\n",
    "print(\"\\nDiversity: \", RecommenderMetrics.Diversity(topNPredicted, simsAlgo))\n",
    "\n",
    "# Measure novelty (average popularity rank of recommendations):\n",
    "print(\"\\nNovelty (average popularity rank): \", RecommenderMetrics.Novelty(topNPredicted, rankings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
