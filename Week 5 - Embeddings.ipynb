{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from the Keras Example https://keras.io/examples/structured_data/collaborative_filtering_movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Embeddings for Recommendation \n",
    "\n",
    "Here we'll see how to train our simple **Dot Product** model, along with our **user embeddings** and **item embeddings** using the **Keras** library. As before, we'll be checking out the **MovieLens** dataset \n",
    "\n",
    "## Loading in the Dataset\n",
    "\n",
    "First we load in the small version of the dataset. As this is a **Collaborative Filtering** approach, we are interested in the **ratings.csv**, which has all over ratings made by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "What we have in the dataset is a list of **userId** and **movieId** pairs loaded into a ``Pandas`` DataFrame. \n",
    "\n",
    "As we said before, you can think of an embedding layer as a **one-hot encoding** layer the size of your **vocabulary**, followed by a **fully connected layer** the size of your embedding. \n",
    "\n",
    "When we make the embedding, we will need a way of mapping back from **indexes** in the **one-hot encoding** back to the ids for the users and movies. \n",
    "\n",
    "### Vocabulary \n",
    "\n",
    "In order to make the vocabulary (all the unique ids), we can use the ``unique()`` function in ``Pandas``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df[\"userId\"].unique().tolist()\n",
    "movie_ids = df[\"movieId\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-sequential list of ids\n",
    "movie_ids[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Comprehensions \n",
    "\n",
    "We've seen ``Dictionaries`` (e.g. when looking at JSON from REST APIs). This is a collection like a ``List``, but instead of using indexes to access data (**values**), we use **keys**. \n",
    "\n",
    "We've also seen ``List Comprehensions``, a short hand way to iterate through an existing collection and make a new ``List``. \n",
    "\n",
    "As we want something where we can use an arbitrary string/number (e.g. a movie or user id) to look up an index, a ``Dictionary`` seems like a good data structure to use. We can declare dictionaries manually (see below), but it would be much quicker and cleaner to use the information we already have to make this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually making the dictionary\n",
    "movie_id_to_index = {\n",
    "    31: 1,\n",
    "    1029: 2,\n",
    "    1061: 3\n",
    "}\n",
    "#Use a movie id to look up an index\n",
    "movie_id_to_index[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the ``List Comprehension``, the ``Dictionary Comprehension`` iterates through a given collection, does some calculation and stores new values in a new collection. \n",
    "\n",
    "In this case, we need to return both a ``Key`` and a ``Value`` for each item. \n",
    "\n",
    "```\n",
    "a = [1,2,3]\n",
    "b = {i:i+1 for i in a} \n",
    "```\n",
    "\n",
    "is the same as \n",
    "\n",
    "```\n",
    "a = [1,2,3]\n",
    "b = {}\n",
    "for i in a:\n",
    "    b[i] = i+1\n",
    "```\n",
    "\n",
    "where we end up with the ``Dictionary``\n",
    "\n",
    "```\n",
    "{\n",
    "    1: 2,\n",
    "    2: 3,\n",
    "    3: 4\n",
    "}\n",
    "```\n",
    "\n",
    "Below, we combine the dictionary comprehension with the ``enumerate()`` function to return the id (x) and the index (i) and store them in a new dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary mapping ids (keys) to indexes (values)\n",
    "user_id_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "movie_id_to_index = {x: i for i, x in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column in the dataframe which contains the appropriate index for each user and movie\n",
    "df[\"user_index\"] = [user_id_to_index[i] for i in df[\"userId\"]]\n",
    "df[\"movie_index\"] = [movie_id_to_index[i] for i in df[\"movieId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Scaling the ratings\n",
    "\n",
    "As is good when working with ``gradient descent``, it helps to have our values on a similar range, and for that to be between 0 and 1. We can use the ``MinMaxScaler`` from ``Scikit-Learn`` to scale our ratings to between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "##Pick the range\n",
    "df[\"rating\"] = MinMaxScaler().fit_transform(df[\"rating\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set\n",
    "\n",
    "We are making a **predictive model** that will take a **user** and **movie** and return a **rating**. \n",
    "\n",
    "For our training, we will make a dataset using the information we already know. In this context, our input feautres (``x``) are the movie and user indexes, and the our output (``y``) is the rating. \n",
    "\n",
    "We make a train - test split of ``10%`` to validate our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Inputs\n",
    "x = df[[\"user_index\", \"movie_index\"]]\n",
    "#Outputs\n",
    "y = df[\"rating\"]\n",
    "#Get train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Custom Model \n",
    "\n",
    "Previously in ``Keras`` we have used to pre-existing layers, connecting them all together in using the [Sequential](https://keras.io/guides/sequential_model/) object. This allows us to fit together layers that pass information forwards in a structure that works for most **Neural Networks**.\n",
    "\n",
    "``Keras`` also has a [Model](https://keras.io/api/models/model/) object which we can **subclass**. Without getting too bogged down in the details of **Object Oriented Programming**, essentially what this means is we can take the **existing functionality** from this object and **override** certain functions to add in custom behaviour.\n",
    "\n",
    "Using the ``Model`` structure, we have something that can take advantage of a lot the things that are built into the ``Keras`` library. It can be trained, can have layers, can have parameters that can be optimised.\n",
    "\n",
    "**But**, we can also add in our own functionality. \n",
    "\n",
    "The two main functions we want to override and these are \n",
    "\n",
    "1. ``def __init__()``\n",
    "    \n",
    "    * This is called **once** when the object is first made. We can use this to define our layers \n",
    "    \n",
    "\n",
    "2. ``def call()``\n",
    "\n",
    "    * This is called everytime we want to make a forwards pass. This means it takes some **inputs** and returns some **outputs**. This is called during training, or for inference on a trained model. \n",
    "    \n",
    "### LouisNet\n",
    "\n",
    "Below, we show an **incredibly simple model**, but it should help you get an intuition for what function is called at when in the training process\n",
    "\n",
    "We can see the ``__init__()`` is called once, and then the ``call()`` is called **once per batch**, where we get the inputs for this batch and return some outputs\n",
    "\n",
    "This model doesnt actually have any parameters to train, its more to demonstrate the subclassing principle in the simplest terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define class and subclass keras.Model\n",
    "class LouisNet(keras.Model):\n",
    "    \n",
    "    #Override __init__()\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LouisNet, self).__init__(**kwargs)\n",
    "        print(\"__init__ called\")\n",
    "    \n",
    "    #Override call()\n",
    "    def call(self, inputs):\n",
    "        tf.print(\"\\nforwards pass (new batch)\")\n",
    "        tf.print(inputs,\"\\n\")\n",
    "        #return the output (its just the input, unchanged)\n",
    "        return inputs\n",
    "\n",
    "#Make a new instance of LouisNet    \n",
    "louisNet = LouisNet()\n",
    "louisNet.compile()\n",
    "#Train\n",
    "louisNet.fit(\n",
    "    x=[[1],[2],[3],[4]],\n",
    "    y=[[5],[6],[7],[8]],\n",
    "    epochs=2,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dot Product Recommender Model\n",
    "\n",
    "Lets remember the model we're trying to make. \n",
    "\n",
    "\n",
    "```\n",
    "Predicted Rating = Dot Product(user_vector, item_vector) + user_bias + item_bias\n",
    "```\n",
    "\n",
    "\n",
    "Our target is to find a vector for each movie and user so that their dot product (+ their biases) is an accurate prediction for the rating that user would make for that movie. \n",
    "\n",
    "Each of these vectors will be contained in a matrix, that we call an **embedding**\n",
    "\n",
    "\n",
    "### The Embedding Layer \n",
    "\n",
    "Again, you can think of an embedding layer as a **one-hot encoding** layer the size of your **vocabulary**, followed by a **fully connected layer** the size of your embedding. \n",
    "\n",
    "Luckily, ```Keras``` has a layer already we can use, all we have to say is \n",
    "\n",
    "1. How many items we have (vocabulary size)\n",
    "\n",
    "2. The size of the embedding \n",
    "\n",
    "You might use something between 10-300, and this is something you will have to tune\n",
    "\n",
    "### New Arguments for ``__init__``\n",
    "\n",
    "Again, we will override the ```__init__()``` function, but this time we will add in some extra arguments. We can use this to pass in \n",
    "\n",
    "1. Number of users \n",
    "\n",
    "2. Number of movies\n",
    "\n",
    "3. Size of Embedding\n",
    "\n",
    "These get passed in when we make the new object \n",
    "\n",
    "```\n",
    "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "\n",
    "```\n",
    "\n",
    "### Saving Variables and ```self```\n",
    "\n",
    "Finally, the last **Object-oriented** concept we'll need allows us to save things within the object. These are sometimes called ``instance variables`` or ``fields``, but the main thing you need to know is **these are like the variables we use all the time to store objects and data**, apart from they belong to the object, and only work within this context \n",
    "\n",
    "We use the keyword ```self``` within the object to refer to itself. We can use this to make layers in the ```__init__()``` function, store them in the object, and then reuse and update them in the ```call()``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the new class\n",
    "class RecommenderNet(keras.Model):\n",
    "    \n",
    "    #Override init with new arguments \n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        #Make an embedding layer for users\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        #Make an embedding layer for user bias\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        #Make an embedding layer for movies\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        #Make an embedding layer for movie bias\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #inputs contains [[user,movie],[user,movie],[user,movie]...]\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        #Dot product \n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # The sigmoid activation forces the rating to between 0 and 1\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \n",
    "\n",
    "Now we can ```compile()``` and ```fit()``` just like we would any model. \n",
    "\n",
    "\n",
    "On every forwards pass (see ``call()`` above)\n",
    "\n",
    "1. We take a batch of ``users`` and ``movies``\n",
    "\n",
    "\n",
    "2. Run them through the normal embedding and bias embedding layers respectively \n",
    "\n",
    "\n",
    "3. Get the vectors for each out \n",
    "\n",
    "\n",
    "4. Get the dot product of the user and movie vectors \n",
    "\n",
    "\n",
    "5. Add the biases \n",
    "\n",
    "\n",
    "6. Run through a sigmoid\n",
    "\n",
    "\n",
    "7. Return!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick Embedding size\n",
    "EMBEDDING_SIZE = 20\n",
    "#Make new object (calls __init__())\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(lr=0.001)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Embeddings \n",
    "\n",
    "We can access the **embedding layers** in our model object, and access the ``trainable_weights``. This is the embedding and we can see is has a shape of ```num_users x EMBEDDING_SIZE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.user_embedding.trainable_weights[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions \n",
    "\n",
    "Now, we can use our trained model to make predictions, and with the predicted ratings, we can pick some recommendations!\n",
    "\n",
    "In order to get the ratings for all movies for a given user, we need to get pass in our data in the form \n",
    "\n",
    "```\n",
    "[\n",
    "    [user_id, movie_1_id],\n",
    "    [user_id, movie_2_id],\n",
    "    [user_id, movie_3_id],\n",
    "    .....\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the movie data so we can map back to names\n",
    "movie_data = pd.read_csv(\"../ml-latest-small/movies.csv\")\n",
    "movie_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions and `argsort()`\n",
    "\n",
    "Once we have the predicted ratings for each film, we need to get the **Top N**\n",
    "\n",
    "Here we use `np.argsort()`, which does the sort based on the **ratings** but returns the **indexes** rather than the **ratings themselves**. We can then use this to look up the `movie_ids` and then the `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 3\n",
    "n = 10\n",
    "#For one user, make a pair with every movie index\n",
    "x = [[user, i] for i in np.arange(num_movies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "predicted_ratings = model.predict(x).flatten()\n",
    "#Get Top-N indexes\n",
    "top_n_indexes = predicted_ratings.argsort()[-n:]\n",
    "#Get Movie Names\n",
    "top_n = [movie_data[movie_data[\"movieId\"]==movie_ids[i]][\"title\"] for i in top_n_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessed Assignment 2\n",
    "\n",
    "Please remember to comment your code clearl, submit ``.ipynb`` \n",
    "\n",
    "## Task 1\n",
    "\n",
    "We're going to ask you take the trained model and write the code to make two metrics - **Diversity** and **Novelty**\n",
    "\n",
    "### Diversity \n",
    "\n",
    "This tells us what the mean diversity (1-similarity, based on movie embeddings) between each film in every users Top 10 films is.  \n",
    "\n",
    "### Novelty \n",
    "\n",
    "This tells us what the mean popularity (e.g. mean rating) of the films in every users Top 10 films is \n",
    "\n",
    "## Task 2\n",
    "\n",
    "Using a dimensionality reduction approach, plot the top 30 best rated films on a 2-D graph based on their movie embeddings "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
