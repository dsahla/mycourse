{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#ONLY FOR USING GPU (LOUIS)\n",
    "import tensorflow as tf\n",
    "def expand():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "expand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - RNNs for Recommending Fashion \n",
    "\n",
    "This week we'll take a look at how we can take the **sequence of purchases** into account when recommending new things. \n",
    "\n",
    "Developing on using **embeddings** (Week 5) and looking at **deep learning** approaches at YouTube (Week 6), we're going to use as dataset of customer interactions from the Fasion Rental site [Rent the Runway](https://www.renttherunway.com/)\n",
    "\n",
    "We'll take an approach adapted from this informative approach recently described by [Decatholon's](https://www.decathlon.co.uk/) engineers as they moved from a **matrix factorisation approach** (similar to what we saw in week 5), to a **Recurrent Neural Network**. The full description is in this [blog post](https://medium.com/decathlondevelopers/building-a-rnn-recommendation-engine-with-tensorflow-505644aa9ff3)\n",
    "\n",
    "###Â The Approach \n",
    "\n",
    "**Decatholon** describe a dataset where they have a list of items purchased by each user, and the date that it was purchased. They aim to leverage this information about the sequence of purchases, as well as the information encoded within the date of each purchase to build a model that can predict future items to recommend. \n",
    "\n",
    "As we'vee seen before **Recurrent Neural Networks** (RNNs) are great at encoding this sequenital information. We also have sparse categorical data for ``item ids``, so we'll also incorporate an **embedding layer** at the beginning. \n",
    "\n",
    "We don't have their data, but we ca find a dataset that has this sequential, dated purchase history by user. We have the **Rent the Runway** dataset that has this, along with loads of other rich metadata about the customers and their experiences of each rental. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the data \n",
    "df = pd.read_json(\"../data/renttherunway_cleaned.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>April 20, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>June 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>None</td>\n",
       "      <td>1063761</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "      <td>party</td>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>None</td>\n",
       "      <td>It was a great time to celebrate the (almost) ...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>December 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>February 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>September 26, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
       "0  fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
       "1  fit   273551       34b   153475  132lbs    10.0          other   \n",
       "2  fit   360448      None  1063761    None    10.0          party   \n",
       "3  fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
       "4  fit   151944       34b   616682  145lbs    10.0        wedding   \n",
       "\n",
       "                                         review_text          body type  \\\n",
       "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2  This hugged in all the right places! It was a ...               None   \n",
       "3  I rented this for my company's black tie award...               pear   \n",
       "4  I have always been petite in my upper body and...           athletic   \n",
       "\n",
       "                                      review_summary category height  size  \\\n",
       "0                               So many compliments!   romper  5' 8\"    14   \n",
       "1                            I felt so glamourous!!!     gown  5' 6\"    12   \n",
       "2  It was a great time to celebrate the (almost) ...   sheath  5' 4\"     4   \n",
       "3   Dress arrived on time and in perfect condition.     dress  5' 5\"     8   \n",
       "4                    Was in love with this dress !!!     gown  5' 9\"    12   \n",
       "\n",
       "     age         review_date  \n",
       "0   28.0      April 20, 2016  \n",
       "1   36.0       June 18, 2013  \n",
       "2  116.0   December 14, 2015  \n",
       "3   34.0   February 12, 2014  \n",
       "4   27.0  September 26, 2016  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192544, 105571, 5850)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many interactions in total, how many unique users, how many unique items \n",
    "len(df), len(df[\"user_id\"].unique()), len(df[\"item_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates \n",
    "\n",
    "Since sequence is important to us, we're going to need to sort the data by review date. Currently, ``Pandas`` sees this column as an ``object``, so we'll use ``pd.to_datetime()`` to convert the string to a date. We can then sort it, and do maths with it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the dates\n",
    "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit                       object\n",
       "user_id                    int64\n",
       "bust size                 object\n",
       "item_id                    int64\n",
       "weight                    object\n",
       "rating                   float64\n",
       "rented for                object\n",
       "review_text               object\n",
       "body type                 object\n",
       "review_summary            object\n",
       "category                  object\n",
       "height                    object\n",
       "size                       int64\n",
       "age                      float64\n",
       "review_date       datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm the data types are correct\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort\n",
    "df = df.sort_values(by=\"review_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the User Sequence \n",
    "\n",
    "Currently, our dataset has each item on a separate row. In order to get the sequence of purchase for each user, we need to format the data. \n",
    "\n",
    "We also take the date in its absolute form and change it to ``days since end of dataset`` (the most recent rental being ``0 days``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df[\"user_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "last_date = df[\"review_date\"].max()\n",
    "\n",
    "for user in users:\n",
    "    ##Get all the items for that user\n",
    "    rows = df[df[\"user_id\"]==user]\n",
    "    \n",
    "    #Get all the item_ids\n",
    "    items = rows[\"item_id\"]\n",
    "    \n",
    "    #Get all the dates\n",
    "    days_since_now = (last_date - rows[\"review_date\"])\n",
    "    days_since_now = np.array([i.days for i in days_since_now])\n",
    "    \n",
    "    #Collect into a dictionary for each user\n",
    "    data.append({\"item_id\":items.values,\"nb_days\":days_since_now})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to a dataframe and save\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>nb_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[125564, 183200]</td>\n",
       "      <td>[2623, 1383]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[126335, 132738, 130259]</td>\n",
       "      <td>[2520, 2096, 1745]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[126335]</td>\n",
       "      <td>[2511]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[125564, 240137, 468020]</td>\n",
       "      <td>[2510, 848, 848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[190529]</td>\n",
       "      <td>[2500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105566</th>\n",
       "      <td>[454564]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105567</th>\n",
       "      <td>[1498329]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105568</th>\n",
       "      <td>[2835159]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105569</th>\n",
       "      <td>[1969604]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105570</th>\n",
       "      <td>[2726766]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105571 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         item_id             nb_days\n",
       "0               [125564, 183200]        [2623, 1383]\n",
       "1       [126335, 132738, 130259]  [2520, 2096, 1745]\n",
       "2                       [126335]              [2511]\n",
       "3       [125564, 240137, 468020]    [2510, 848, 848]\n",
       "4                       [190529]              [2500]\n",
       "...                          ...                 ...\n",
       "105566                  [454564]                 [3]\n",
       "105567                 [1498329]                 [3]\n",
       "105568                 [2835159]                 [3]\n",
       "105569                 [1969604]                 [3]\n",
       "105570                 [2726766]                 [1]\n",
       "\n",
       "[105571 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Dataset \n",
    "\n",
    "Now we need to make the actual **training set** that we will use for our model. \n",
    "\n",
    "Remember, the purpose of this model is to learn to predict the **next item rented in the sequence**. So for our input we will have \n",
    "\n",
    " * [item1, item2, item3, item4, ..., itemN]\n",
    " \n",
    " * [days1, days2, days3, days4, ..., daysN]\n",
    " \n",
    " \n",
    "And for our output we will have **the next item in the sequence** \n",
    "  \n",
    " \n",
    " * [itemN+1]\n",
    " \n",
    " \n",
    "We also need our sequences to all be **of equal length**, and because this dataset doesn't have loads of really long sequences, we don't want to throw away stuff below a threshold! So, instead we **zero padd** the end of each sequence if its not as long as the maximum length we have picked (in this case ``5``)\n",
    "\n",
    "* [0, 0, item1, item2, ..., itemN]\n",
    " \n",
    "* [0, 0, days1, days2, ..., daysN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into examples of 5 last things + 1, if less than 5, zero pad\n",
    "training_set = []\n",
    "max_len = 5\n",
    "for _, user in data.iterrows():\n",
    "    i = 0\n",
    "    #Get all items for each user \n",
    "    items = user[\"item_id\"]\n",
    "    num_items = len(items)\n",
    "    #If only one item purchased, there is no sequence! We need at least 2\n",
    "    #I've found better accuracy limiting to min 3\n",
    "    if num_items > 2:\n",
    "        nb_days = np.array(user[\"nb_days\"])\n",
    "        end = False\n",
    "        #Cycle over items, taking windows of 5 and moving forwards by 1 each time\n",
    "        while not end:\n",
    "            \n",
    "            #If we're off the end of the list, break out of the loop\n",
    "            target = i + max_len\n",
    "            if target >= num_items - 1:\n",
    "                end = True\n",
    "                target = num_items - 1\n",
    "            \n",
    "            #Get the input item and day features, zero padding\n",
    "            input_items = pad_sequences([items[i:target]], max_len)[0]\n",
    "            days = pad_sequences([nb_days[i:target]], max_len)[0]\n",
    "            \n",
    "            #Get the adjusted seqeunce (shifted by one) for the target\n",
    "            target_items = np.concatenate((input_items[1:],[items[target]]))\n",
    "            #target_items = [items[target]]\n",
    "            \n",
    "            #Add to the dataset\n",
    "            row = {\n",
    "                \"item_id\":input_items,\n",
    "                \"nb_days\":days,\n",
    "                \"target\":target_items\n",
    "            }\n",
    "            training_set.append(row)\n",
    "            \n",
    "            #Increment pointer\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.DataFrame(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34588"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>nb_days</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 126335, 132738]</td>\n",
       "      <td>[0, 0, 0, 2520, 2096]</td>\n",
       "      <td>[0, 0, 126335, 132738, 130259]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 125564, 240137]</td>\n",
       "      <td>[0, 0, 0, 2510, 848]</td>\n",
       "      <td>[0, 0, 125564, 240137, 468020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 126335, 531077]</td>\n",
       "      <td>[0, 0, 0, 2478, 992]</td>\n",
       "      <td>[0, 0, 126335, 531077, 253667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 126335, 1338469, 2261828]</td>\n",
       "      <td>[0, 0, 2474, 1105, 1105]</td>\n",
       "      <td>[0, 126335, 1338469, 2261828, 1846462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 126335, 180014, 833666]</td>\n",
       "      <td>[0, 0, 2458, 1119, 735]</td>\n",
       "      <td>[0, 126335, 180014, 833666, 640839]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34583</th>\n",
       "      <td>[0, 0, 0, 715164, 2720289]</td>\n",
       "      <td>[0, 0, 0, 4, 4]</td>\n",
       "      <td>[0, 0, 715164, 2720289, 2665815]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34584</th>\n",
       "      <td>[0, 0, 0, 872442, 322704]</td>\n",
       "      <td>[0, 0, 0, 4, 4]</td>\n",
       "      <td>[0, 0, 872442, 322704, 844580]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34585</th>\n",
       "      <td>[0, 0, 2893615, 2072280, 2675545]</td>\n",
       "      <td>[0, 0, 4, 4, 4]</td>\n",
       "      <td>[0, 2893615, 2072280, 2675545, 1434889]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34586</th>\n",
       "      <td>[0, 0, 0, 1252971, 2945301]</td>\n",
       "      <td>[0, 0, 0, 4, 4]</td>\n",
       "      <td>[0, 0, 1252971, 2945301, 1793377]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34587</th>\n",
       "      <td>[0, 0, 0, 2237227, 2125981]</td>\n",
       "      <td>[0, 0, 0, 3, 3]</td>\n",
       "      <td>[0, 0, 2237227, 2125981, 2451150]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34588 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 item_id                   nb_days  \\\n",
       "0              [0, 0, 0, 126335, 132738]     [0, 0, 0, 2520, 2096]   \n",
       "1              [0, 0, 0, 125564, 240137]      [0, 0, 0, 2510, 848]   \n",
       "2              [0, 0, 0, 126335, 531077]      [0, 0, 0, 2478, 992]   \n",
       "3       [0, 0, 126335, 1338469, 2261828]  [0, 0, 2474, 1105, 1105]   \n",
       "4         [0, 0, 126335, 180014, 833666]   [0, 0, 2458, 1119, 735]   \n",
       "...                                  ...                       ...   \n",
       "34583         [0, 0, 0, 715164, 2720289]           [0, 0, 0, 4, 4]   \n",
       "34584          [0, 0, 0, 872442, 322704]           [0, 0, 0, 4, 4]   \n",
       "34585  [0, 0, 2893615, 2072280, 2675545]           [0, 0, 4, 4, 4]   \n",
       "34586        [0, 0, 0, 1252971, 2945301]           [0, 0, 0, 4, 4]   \n",
       "34587        [0, 0, 0, 2237227, 2125981]           [0, 0, 0, 3, 3]   \n",
       "\n",
       "                                        target  \n",
       "0               [0, 0, 126335, 132738, 130259]  \n",
       "1               [0, 0, 125564, 240137, 468020]  \n",
       "2               [0, 0, 126335, 531077, 253667]  \n",
       "3       [0, 126335, 1338469, 2261828, 1846462]  \n",
       "4          [0, 126335, 180014, 833666, 640839]  \n",
       "...                                        ...  \n",
       "34583         [0, 0, 715164, 2720289, 2665815]  \n",
       "34584           [0, 0, 872442, 322704, 844580]  \n",
       "34585  [0, 2893615, 2072280, 2675545, 1434889]  \n",
       "34586        [0, 0, 1252971, 2945301, 1793377]  \n",
       "34587        [0, 0, 2237227, 2125981, 2451150]  \n",
       "\n",
       "[34588 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "### Item Ids\n",
    "\n",
    "Currently the ``item_ids`` are arbitrary and relate to the **Rent the Runway** catalogue. We are going to encode the in an **embedding** so need to make them indexes from ``0 -> num_items``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the unique items from the data set\n",
    "all_input_items = np.array([i for i in training_set[\"item_id\"].values]).flatten()\n",
    "all_target_items = np.array([i for i in training_set[\"target\"].values]).flatten()\n",
    "all_items = np.concatenate((all_input_items, all_target_items))\n",
    "unique_items = np.unique(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a look up dictionary from item_id to index\n",
    "item_to_index = {item_id:i for i, item_id in enumerate(unique_items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swap out the new ids and convert to 2d arrays for use in training \n",
    "item_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"item_id\"]], dtype=int)\n",
    "target_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"target\"]], dtype=int)\n",
    "days = np.array([np.array([j for j in i],dtype=int) for i in training_set[\"nb_days\"]],dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketting Dates\n",
    "\n",
    "One interesting approach they have taken at Decathalon is to **bucket** days and then **learn an embedding**, almost treating it as a **categorical variable**. \n",
    "\n",
    "To do this, we use ``tf.keras.layers.experimental.preprocessing.Discretization()`` to separate the days into **100 equally spaced bins**, with one more for the **zero padding** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    }
   ],
   "source": [
    "#Get min and max\n",
    "days_min = days.min()\n",
    "days_max = days.max()\n",
    "\n",
    "#Generate 100 equally spaced boundaries \n",
    "boundaries = list(np.linspace(days_min, days_max,100,dtype=int))\n",
    "boundaries.insert(0,0)\n",
    "boundaries[1] = 1\n",
    "\n",
    "#Bucket the day features\n",
    "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
    "    bins=boundaries)\n",
    "bucket_days = discretize_layer(days).numpy() - 1\n",
    "bucket_days = bucket_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0, 2520, 2096],\n",
       "       [   0,    0,    0, ...,    0, 2510,  848],\n",
       "       [   0,    0,    0, ...,    0, 2478,  992],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,    4,    4],\n",
       "       [   0,    0,    0, ...,    0,    4,    4],\n",
       "       [   0,    0,    0, ...,    0,    3,    3]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0, 100,  84],\n",
       "       [  0,   0,   0, ...,   0, 100,  35],\n",
       "       [  0,   0,   0, ...,   0,  99,  40],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   2,   2,   2],\n",
       "       [  0,   0,   0, ...,   0,   2,   2],\n",
       "       [  0,   0,   0, ...,   0,   2,   2]], dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets \n",
    "\n",
    "Next we split the data into training and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate and shuffle the indexes\n",
    "total = len(training_set)\n",
    "all_indexes = np.arange(total)\n",
    "np.random.shuffle(all_indexes)\n",
    "\n",
    "#Split the indexes\n",
    "split = 0.9\n",
    "train_indexes = all_indexes[:int(total*split)]\n",
    "test_indexes = all_indexes[int(total*split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary for the inputs \n",
    "train_x = {'item_id': item_indexes[train_indexes],\n",
    "          'nb_days': bucket_days[train_indexes]}\n",
    "test_x = {'item_id': item_indexes[test_indexes],\n",
    "          'nb_days': bucket_days[test_indexes]}\n",
    "\n",
    "#Make an array for the outputs \n",
    "train_y = target_indexes[train_indexes]\n",
    "test_y = target_indexes[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': array([[   0,    0,    0, ...,  145, 1705, 2357],\n",
       "        [3126, 1764,  315, ..., 3407, 4145,  458],\n",
       "        [5408, 3923, 5160, ...,  722, 1066, 2175],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    0,  101,  154],\n",
       "        [1041, 1941, 1403, ..., 4106, 3607, 1453],\n",
       "        [3911, 5262, 5070, ..., 2835, 1346, 5518]]),\n",
       " 'nb_days': array([[ 0,  0,  0, ..., 50, 18, 18],\n",
       "        [37, 37, 37, ..., 34, 31, 25],\n",
       "        [12, 11, 11, ..., 11, 10, 10],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0, 55, 53],\n",
       "        [10, 10,  9, ...,  7,  7,  7],\n",
       "        [11, 11, 11, ..., 11, 11, 11]], dtype=int32)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model \n",
    "\n",
    "Previously we have used ``Keras's Sequential API``, where we first make a model then **sequentially add layers to it** one by one. This works because each layer only has one input, and outputs directly into the next. \n",
    "\n",
    "However, whilst this is broadly true of our model, its not strictly true. Our **Embedding layers** both feed into the **Concatenate layer**. So instead, we will use the [Keras Functional API](https://keras.io/guides/functional_api/).\n",
    "\n",
    "As the documentation says \n",
    "\n",
    "```\n",
    "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "```\n",
    "\n",
    "Essentially, when we build up the ``network graph``, instead of **adding things to the model**, we **specify input layer** we want this new layer to have. \n",
    "\n",
    "Here, we make a layer (``x``), and then a layer(``output``), specifying that ``x`` is in the input for ``output``\n",
    "\n",
    "```\n",
    "x = layers.Dense(64, activation=\"relu\")\n",
    "outputs = layers.Dense(10)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyper parameters \n",
    "item_vocab_size = len(unique_items)\n",
    "hp = {\n",
    "    \"embedding_item\":100,\n",
    "    \"embedding_nb_days\":20,\n",
    "    \"rnn_units_cat\":[1024,512],\n",
    "    \"learning_rate\":0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``tf.keras.Input()``\n",
    "\n",
    "The [Input Layer](https://keras.io/api/layers/core_layers/input/) sits at the base of our ``Model`` and takes does what it says on the tin - Takes the input. \n",
    "\n",
    "We specfiy a dictionary of inputs to match our dictionaries we made in the **training set**. This means we have **two separate inputs**, that feed into **two separate embedding layers**.\n",
    "\n",
    "We give them a ``batch_input_shape`` of ``[None, max_len]``, so that it know each batch will be a sequence of ``max_len`` (in our ase 5) items, but the batch_size itself isn't decided until we ``compile()`` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "inputs['item_id'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "                                   name='item_id', dtype=tf.int32)\n",
    "\n",
    "# nb_days bucketized\n",
    "inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "                                   name='nb_days', dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``tf.keras.layers.Embedding()``\n",
    "\n",
    "Then we add the embedding layers, each time specifying which item from the **input dictionary** to take as input. \n",
    "\n",
    "The **item embedding** takes an input the size of the **number of unique items (vocab size)** and learns a mapping to a denser embedding of a given size. \n",
    "\n",
    "The **days embedding** takes an input the size of the **number of buckets + 1 (for zero padding)** and learns a mapping to a denser embedding of a given size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
    "                                           output_dim=hp.get('embedding_item'),\n",
    "                                           name='embedding_item'\n",
    "                                          )(inputs['item_id'])\n",
    "# nbins=100, +1 for zero padding\n",
    "embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
    "                                              output_dim=hp.get('embedding_nb_days'),\n",
    "                                              name='embedding_nb_days'\n",
    "                                             )(inputs['nb_days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `` tf.keras.layers.Concatenate()``\n",
    "\n",
    "We then concatentate embedding layers into one layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embedding layers\n",
    "concat_embedding_input = tf.keras.layers.Concatenate(\n",
    " name='concat_embedding_input')([embedding_item, embedding_nb_days])\n",
    "\n",
    "concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
    " name='batchnorm_inputs')(concat_embedding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layers \n",
    "\n",
    "When then put in the ``tf.keras.layers.LSTM()`` layer, with a ``tf.keras.layers.BatchNormalization()`` either side. \n",
    "\n",
    "More on that in the lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = concat_embedding_input\n",
    "\n",
    "for i, num_units in enumerate(hp.get('rnn_units_cat')):\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=num_units,\n",
    "                                   return_sequences=True,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat' + str(i)\n",
    "                                   )(input_layer)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm' + str(i))(rnn)\n",
    "    \n",
    "    input_layer = rnn\n",
    "\n",
    "# create encoding padding mask\n",
    "encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
    "\n",
    "# Self attention so key=value in inputs\n",
    "att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                name='attention')(inputs=[rnn, rnn],\n",
    "                                                  mask=[encoding_padding_mask,\n",
    "                                                        encoding_padding_mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output\n",
    "\n",
    "Finally, we bring it all together in a ``tf.keras.layers.Dense()`` **softmax layer**. This means that the output of this layer will be \n",
    "\n",
    "``\n",
    "[batch_size x max_len x item_vocab_size]\n",
    "``\n",
    "\n",
    "Where each sequence in the batch is a ``[max_len x item_vocab_size]`` tensor telling us the probability of that item in the catalogue being next in the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last layer is a fully connected one\n",
    "output = tf.keras.layers.Dense(item_vocab_size, activation = tf.nn.softmax, name='output')(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Loss Function \n",
    "\n",
    "We write a custom loss function. This is necessary because we need to again mask out the 0s to stop the model optimising towards them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "def loss_function(real, pred):\n",
    "    loss = SparseCategoricalCrossentropy()(real, pred)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``tf.keras.Model()``\n",
    "\n",
    "Finally, we're ready to join the ``Inputs`` and the ``Outputs`` into a ``Model()`` object and ``compile()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "    loss=loss_function,\n",
    "    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "item_id (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nb_days (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item (Embedding)      (None, 8, 100)       570100      item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_nb_days (Embedding)   (None, 8, 20)        2020        nb_days[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_embedding_input (Concate (None, 8, 120)       0           embedding_item[0][0]             \n",
      "                                                                 embedding_nb_days[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_inputs (BatchNormaliz (None, 8, 120)       480         concat_embedding_input[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_cat0 (LSTM)                (None, 8, 1024)      4689920     batchnorm_inputs[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_lstm0 (BatchNormaliza (None, 8, 1024)      4096        LSTM_cat0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_cat1 (LSTM)                (None, 8, 512)       3147776     batchnorm_lstm0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.equal_1 (TFOpLambda)    (None, 8)            0           item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_lstm1 (BatchNormaliza (None, 8, 512)       2048        LSTM_cat1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.logical_not_1 (TFOpLamb (None, 8)            0           tf.math.equal_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 8, 512)       0           batchnorm_lstm1[0][0]            \n",
      "                                                                 batchnorm_lstm1[0][0]            \n",
      "                                                                 tf.math.logical_not_1[0][0]      \n",
      "                                                                 tf.math.logical_not_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 8, 5701)      2924613     attention[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,341,053\n",
      "Trainable params: 11,337,741\n",
      "Non-trainable params: 3,312\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 4s 50ms/step - loss: 4.4855 - sparse_categorical_accuracy: 0.0016 - val_loss: 4.2407 - val_sparse_categorical_accuracy: 0.0011\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 3.8160 - sparse_categorical_accuracy: 0.0135 - val_loss: 4.0391 - val_sparse_categorical_accuracy: 9.2385e-04\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 3.2511 - sparse_categorical_accuracy: 0.1224 - val_loss: 3.7992 - val_sparse_categorical_accuracy: 0.0067\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 2.6683 - sparse_categorical_accuracy: 0.3230 - val_loss: 3.4410 - val_sparse_categorical_accuracy: 0.0358\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 2.2450 - sparse_categorical_accuracy: 0.4740 - val_loss: 2.9505 - val_sparse_categorical_accuracy: 0.1409\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.9719 - sparse_categorical_accuracy: 0.5498 - val_loss: 2.5460 - val_sparse_categorical_accuracy: 0.2820\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.7863 - sparse_categorical_accuracy: 0.5969 - val_loss: 2.2110 - val_sparse_categorical_accuracy: 0.4446\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.6105 - sparse_categorical_accuracy: 0.6334 - val_loss: 2.0566 - val_sparse_categorical_accuracy: 0.4966\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.4699 - sparse_categorical_accuracy: 0.6473 - val_loss: 1.9709 - val_sparse_categorical_accuracy: 0.5266\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.3448 - sparse_categorical_accuracy: 0.6683 - val_loss: 1.9261 - val_sparse_categorical_accuracy: 0.5445\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 1.2277 - sparse_categorical_accuracy: 0.6817 - val_loss: 1.9111 - val_sparse_categorical_accuracy: 0.5474\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 1.1255 - sparse_categorical_accuracy: 0.6920 - val_loss: 1.9145 - val_sparse_categorical_accuracy: 0.5468\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 1.0461 - sparse_categorical_accuracy: 0.6984 - val_loss: 1.9238 - val_sparse_categorical_accuracy: 0.5417\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.9765 - sparse_categorical_accuracy: 0.7014 - val_loss: 1.9587 - val_sparse_categorical_accuracy: 0.5416\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.9111 - sparse_categorical_accuracy: 0.7147 - val_loss: 1.9818 - val_sparse_categorical_accuracy: 0.5476\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.8508 - sparse_categorical_accuracy: 0.7384 - val_loss: 2.0189 - val_sparse_categorical_accuracy: 0.5503\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.7967 - sparse_categorical_accuracy: 0.7509 - val_loss: 2.0664 - val_sparse_categorical_accuracy: 0.5520\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.7526 - sparse_categorical_accuracy: 0.7685 - val_loss: 2.0873 - val_sparse_categorical_accuracy: 0.5583\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.7236 - sparse_categorical_accuracy: 0.7780 - val_loss: 2.1436 - val_sparse_categorical_accuracy: 0.5616\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.6738 - sparse_categorical_accuracy: 0.8015 - val_loss: 2.2190 - val_sparse_categorical_accuracy: 0.5612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x,train_y,\n",
    "                    epochs=20, \n",
    "                    verbose=1,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for whole test set\n",
    "results = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2779, 8, 5701)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exmples x max_len x vocab size\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5701)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_len x vocab size\n",
    "results[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get index of highest prob in item vocab\n",
    "results[100][-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5), dtype=tf.int32, name='item_id'), name='item_id', description=\"created by layer 'item_id'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5), dtype=tf.int32, name='nb_days'), name='nb_days', description=\"created by layer 'nb_days'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    }
   ],
   "source": [
    "results = model.predict([{\"item_id\":test_x[\"item_id\"][0],\"nb_days\":test_x[\"nb_days\"][0]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3174"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_item = {v:k for k,v in item_to_index.items()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1698166"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_item[3174]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
